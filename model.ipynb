{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arcregeneráció rossz minőségű képekből"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Szükséges modulok importálása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'face_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fe6c013e8352>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'face_recognition'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"G:\\\\Anaconda\\\\envs\\\\torch_env\")\n",
    "import os\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import *\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset objektum az autoenkóder adatbázisának."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the maximum number of pictures included in each epoch\n",
    "#TRAIN_SIZE = 5000\n",
    "\n",
    "class AutoEncoderDataset(Dataset):\n",
    "    def __init__(self, pictures):\n",
    "        self.pictures = pictures\n",
    "        self.counter = 0\n",
    "    def __getitem__(self, index):\n",
    "        self.counter += 1\n",
    "        if self.counter > len(self.pictures)-1:\n",
    "            self.counter = 0\n",
    "        return self.pictures[self.counter][1], self.pictures[self.counter][0]\n",
    "    def __len__(self):\n",
    "        return len(self.pictures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olvassunk be az igazi képeket a Diszkriminátor számára."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorDataset(Dataset):\n",
    "    def __init__(self, TRAIN_SIZE):\n",
    "        self.max_counter = TRAIN_SIZE\n",
    "        self.pictures = []\n",
    "        for pic in os.listdir(\"faces\"):\n",
    "            image = cv2.imread(\"faces/\"+pic)\n",
    "            image = torch.Tensor(image).permute(2, 1, 0)\n",
    "            image = (2/255) * image - torch.ones(3, 128, 128)\n",
    "            self.pictures.append(image)\n",
    "        self.fakes = []\n",
    "        self.counter = 0\n",
    "    def __getitem__(self, index):\n",
    "        self.counter += 1\n",
    "        if self.counter > self.max_counter-1:\n",
    "            self.counter = 0\n",
    "        if self.counter % 2:\n",
    "            return self.pictures[self.counter//2], 0\n",
    "        return self.fakes[self.counter//2], 1\n",
    "    def __len__(self):\n",
    "        return len(self.pictures)+len(self.fakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segédfüggvény a beolvasás állapotának kijelzésére. Ez azért fontos, mert nagy mennyiségű adat áll rendelkezésre, így a beolvasás gyakran lassú, és jó tudni, hogy halad-e, illetve mennyi van hátra ebből a lépésből."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olvassuk be a képeket egy Dataset tárolóobjektumba, és pixelesítsük. Utóbbinál fontos, hogy milyen interpolációt választunk. Mivel a háló erre is rá tud tanulni, ezért a legegyszerűbb interpolációt választjuk, hogy ne vezessük vele félre. Beolvasáskor normáljuk is a képeket a numerikus konvergencia támogatásáért.\n",
    "Használjunk adat augmentációt a pontosság és az adatmennyiség növelésére. Augmentációnak a tükrözést választottam, mivel ez mindenképp megfelelő képet ad eredményül. Osszuk szét az adatot tanító-, teszt- és validációs adatbázisok között, 70:15:15 arányban. Használjunk az ellenőrzés kedvéért egy meghatározott random magot. Rendezzük a képeket kötegekbe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size = 32, TRAIN_SIZE=5000):\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # reading pictures\n",
    "    pictures = []\n",
    "    for pic in log_progress(os.listdir(\"faces\"), every=100):\n",
    "        image = cv2.imread(\"faces/\"+pic)\n",
    "        image2 = image[:, ::-1, :].copy()\n",
    "            \n",
    "        #real images\n",
    "        real = torch.Tensor(image).permute(2, 1, 0)\n",
    "        real = (2/255) * real - torch.ones(3, 128, 128)\n",
    "        real2 = torch.Tensor(image2).permute(2, 1, 0)\n",
    "        real2 = (2/255) * real2 - torch.ones(3, 128, 128)\n",
    "            \n",
    "        # pixelised image2\n",
    "        pixel = cv2.resize(image, (16, 16), interpolation=cv2.INTER_LINEAR)\n",
    "        pixel = torch.Tensor(pixel).permute(2, 1, 0)\n",
    "        pixel = (2/255) * pixel - torch.ones(3, 16, 16)\n",
    "        pixel2 = cv2.resize(image2, (16, 16), interpolation=cv2.INTER_LINEAR)\n",
    "        pixel2 = torch.Tensor(pixel2).permute(2, 1, 0)\n",
    "        pixel2 = (2/255) * pixel2 - torch.ones(3, 16, 16)\n",
    "            \n",
    "        # adding pictures to dataloader\n",
    "        pictures.append((real, pixel))\n",
    "        pictures.append((real2, pixel2))\n",
    "    np.random.shuffle(pictures)\n",
    "\n",
    "    train_dataset = AutoEncoderDataset(pictures[:int(len(pictures)*0.7)])\n",
    "    test_dataset = AutoEncoderDataset(pictures[int(len(pictures)*0.7):int(len(pictures)*0.85)])\n",
    "    validation_dataset = AutoEncoderDataset(pictures[int(len(pictures)*0.85):])\n",
    "\n",
    "    size = min(TRAIN_SIZE, len(pictures))\n",
    "    print(\"Epoch size:\", size)\n",
    "    sampler = SubsetRandomSampler(range(size))\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "    validationloader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size)\n",
    "    \n",
    "    discriminator_dataset = DiscriminatorDataset(TRAIN_SIZE)\n",
    "    sampler = SubsetRandomSampler(range(size))\n",
    "    disc_loader = torch.utils.data.DataLoader(discriminator_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    \n",
    "    print(len(pictures), \"images loaded\")\n",
    "    return trainloader, testloader, validationloader, discriminator_dataset, disc_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder osztály az arcok generálására. A normál autoencoderektől eltér abban, hogy nem szimmetrikus, hiszen rosszabb minőségből jobbat kell előállítania, így a generált kép szükségszerűen nagyobb, mint az eredeti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diszkriminátor osztály. Ennek feladata hasonlít a GAN hálók diszkriminátorára, vagyis azt mondja meg, hogy mennyire hasonlít egy kép egy igazi emberi arcra, és mennyire egy generáltra. Ez az eredmény az autoencoder loss függvényében tölt be szerepet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        ndf = 32\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 128 x 128\n",
    "            nn.Conv2d(3, ndf, 4, 4, 1, bias=False),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x).squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saját loss függvény az autoencoder számára. A hiba számítása az alábbi képlettel írható fel:\n",
    "### (output - target)^2 * (rate + (1 - rate) * D(output))\n",
    "\n",
    ", ahol D(output) a diszkriminátor háló eredménye 0 és 1 közé normalizálva. Ez garantálja, hogy a háló ne csak az eredeti képre hasonlító elmosott képet generáljon, hanem emberi arcra is - még ha az eltér a visszaállítandó képtől is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_loss(output, target, discriminator, ratio=0.5):\n",
    "    d = discriminator(output)\n",
    "    loss = 0\n",
    "    for i in range(len(d)):\n",
    "        loss += torch.mean((output[i]-target[i])**2)*(d[i]*(1-ratio)+ratio)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanító függvény. Először az autoencodert tanítjuk, majd annak a frissen generált képeivel tanítjuk a diszkriminátor hálót is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(autoencoder, discriminator, trainloader, discriminator_dataset, disc_loader,\n",
    "          NUM_EPOCH, ratio, steps_without_disc, TRAIN_SIZE, AUTO_LR, DISC_LR, wd):\n",
    "    \n",
    "    train_loss = []\n",
    "    disc_loss = [0 for i in range(5)]\n",
    "    \n",
    "    # creating components\n",
    "    mse_loss = nn.MSELoss()\n",
    "    auto_optimizer = optim.Adam(autoencoder.parameters(), lr=AUTO_LR)\n",
    "    auto_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(auto_optimizer, NUM_EPOCH)\n",
    "    disc_optimizer = optim.Adam(discriminator.parameters(), lr=DISC_LR, weight_decay=wd)\n",
    "    disc_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(disc_optimizer, NUM_EPOCH)\n",
    "    disc_criterion = nn.BCELoss()\n",
    "    if torch.cuda.is_available():\n",
    "        mse_loss = mse_loss.cuda()\n",
    "        disc_criterion = disc_criterion.cuda()\n",
    "    \n",
    "    # training the network\n",
    "    autoencoder.train(mode=True)\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        # training the autoencoder\n",
    "        \n",
    "        generated_images = []\n",
    "        running_loss = 0.0\n",
    "        for data in trainloader:\n",
    "            pixelised, real = data\n",
    "            if torch.cuda.is_available():\n",
    "                pixelised = pixelised.cuda()\n",
    "                real = real.cuda()\n",
    "            auto_optimizer.zero_grad()\n",
    "            outputs = autoencoder(pixelised)\n",
    "            if epoch < steps_without_disc+1:\n",
    "                loss = mse_loss(outputs, real)\n",
    "            else:\n",
    "                loss = auto_loss(outputs, real, discriminator, ratio)\n",
    "            loss.backward()\n",
    "            auto_optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            for i in outputs:\n",
    "                generated_images.append(i.detach().cpu())\n",
    "        \n",
    "        loss = running_loss / len(trainloader)\n",
    "        train_loss.append(loss)\n",
    "        print('Epoch {} of {}, Train Loss: {:.6f}'.format(epoch+1, NUM_EPOCH, loss))\n",
    "        auto_scheduler.step()\n",
    "        \n",
    "        if epoch < steps_without_disc:\n",
    "            continue\n",
    "        \n",
    "        #updating the discriminator dataset with the freshly generated faces\n",
    "        generated_images = random.sample(generated_images, int(TRAIN_SIZE/2))\n",
    "        discriminator_dataset.fakes = generated_images\n",
    "        \n",
    "        # training the discriminator\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data in disc_loader:\n",
    "            images, labels = data\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            disc_optimizer.zero_grad()\n",
    "            outputs = discriminator(images)\n",
    "            loss = disc_criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            disc_optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # calculating accuracy for discriminator\n",
    "            predictions = torch.round(outputs.data.squeeze())\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += images.size(0)\n",
    "        print('\\tDiscriminator Accuracy: {:.6f}'.format(correct/total))\n",
    "        \n",
    "        loss = running_loss / len(disc_loader)\n",
    "        disc_loss.append(loss)\n",
    "        print('\\tDiscriminator Loss: {:.6f}'.format(loss))\n",
    "        disc_scheduler.step()\n",
    "    \n",
    "    for i in range(steps_without_disc):\n",
    "        disc_loss[i] = disc_loss[steps_without_disc]\n",
    "    return train_loss, disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tesztelő függvény, ami a validációra is alkalmas. Fontos, hogy elkülönüljön a teszt és a validáció, hiszen amikor teszteredményekre optimalizáljuk a hiperparamétereket, akkor ezzel be is szennyezzük a tesztadatokat - előfordulhat, hogy a paraméterek állítgatásával manuálisan megtalálunk egy véletlenül nagyon jó eredményt, ami azonban csak a tesztadatok sajátos eloszlása miatt ilyen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(autoencoder, loader):\n",
    "    generated_images = []\n",
    "    mse_loss = nn.MSELoss()\n",
    "    if torch.cuda.is_available():\n",
    "        mse_loss = mse_loss.cuda()\n",
    "    \n",
    "    # getting the MSE loss\n",
    "    autoencoder.train(mode=False)\n",
    "    autoencoder_loss = 0.0\n",
    "    for data in loader:\n",
    "        pixelised, real = data\n",
    "        if torch.cuda.is_available():\n",
    "            pixelised = pixelised.cuda()\n",
    "            real = real.cuda()\n",
    "        outputs = autoencoder(pixelised)\n",
    "        autoencoder_loss += mse_loss(outputs, real).item()\n",
    "        for i in range(len(outputs)):\n",
    "            generated = outputs[i].detach().cpu()\n",
    "            generated = torch.round((generated + torch.ones(3, 128, 128))*0.5*255).type(torch.ByteTensor)\n",
    "            generated = generated.numpy().transpose(2, 1, 0)\n",
    "            original = pixelised[i].cpu()\n",
    "            original = torch.round((original + torch.ones(3, 16, 16))*0.5*255).type(torch.ByteTensor)\n",
    "            original = original.numpy().transpose(2, 1, 0)\n",
    "            generated_images.append( (generated, original) )\n",
    "    autoencoder_loss = 1 - torch.sigmoid( torch.tensor(autoencoder_loss / len(loader)) ).item()\n",
    "    \n",
    "    # getting the reality score\n",
    "    reality_score = 0.0\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    min = 100000\n",
    "    max = 0\n",
    "    \"\"\"\n",
    "    for i in generated_images:\n",
    "        face_encoding0 = face_recognition.face_encodings(i[0])\n",
    "        if not face_encoding0:\n",
    "            continue\n",
    "        face_encoding1 = face_recognition.face_encodings(i[1])\n",
    "        if not face_encoding1:\n",
    "            continue\n",
    "        results = face_recognition.face_distance([face_encoding1[0]], face_encoding0[0])\n",
    "        reality_score += results\n",
    "        \"\"\"\n",
    "        count += 1\n",
    "        if results > max:\n",
    "            max = results\n",
    "        if results < min:\n",
    "            min = results\n",
    "        \"\"\"\n",
    "        reality_score /= len(generated_images)\n",
    "    print(reality_score)\n",
    "    #print(count, min, max)\n",
    "    \n",
    "    return autoencoder_loss + reality_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Egyéb segédfüggvények."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_tensor(t, size=128):\n",
    "    t = t[0].cpu().detach()\n",
    "    t = (t + torch.ones(3, size, size))*0.5\n",
    "    t = t.numpy().transpose(2, 1, 0)\n",
    "    plt.imshow(cv2.cvtColor(t, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    \n",
    "def redraw(net, pic):\n",
    "    if torch.cuda.is_available():\n",
    "        net = net.cuda()\n",
    "        pic = pic.cuda()\n",
    "    net.train(mode=False)\n",
    "    out = net(pic)\n",
    "    print_tensor(out)\n",
    "    #print(torch.max(out), torch.min(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the maximum number of pictures included in each epoch\n",
    "TRAIN_SIZE = 1000\n",
    "AUTO_LR = 1e-3\n",
    "DISC_LR = 1e-4\n",
    "NUM_EPOCH = 10\n",
    "WD = 1e-8\n",
    "trainloader, testloader, validationloader, discriminator_dataset, disc_loader = get_data(32, TRAIN_SIZE)\n",
    "autoencoder = Autoencoder()\n",
    "if torch.cuda.is_available():\n",
    "    autoencoder = autoencoder.cuda()\n",
    "discriminator = Discriminator()\n",
    "if torch.cuda.is_available():\n",
    "    discriminator = discriminator.cuda()\n",
    "train_loss, disc_loss = train(\n",
    "    autoencoder, discriminator, trainloader, discriminator_dataset, disc_loader, NUM_EPOCH, ratio=0.5,\n",
    "    steps_without_disc=4, TRAIN_SIZE=TRAIN_SIZE, AUTO_LR=AUTO_LR, DISC_LR=DISC_LR, wd=WD)\n",
    "plt.figure()\n",
    "plt.plot(train_loss)\n",
    "plt.title('Train and Discriminator Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(disc_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(get_result(autoencoder, testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
